{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96816ed7-b08a-4ca3-abb9-f99880c3535d"}}},{"cell_type":"code","source":["# File location and type\nfile_location1 = \"/FileStore/tables/iris__1_-3.csv\"\nfile_location2 = \"/FileStore/tables/glass__1_-3.csv\"\n\nfile_location3 = \"/FileStore/tables/parkinsons__1_-3.csv\"\n\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\niris = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location1)\n\nglass = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location2)\n\nparkinsons = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location3)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import math\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom pyspark.sql.functions import lit\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.functions import vector_to_array\nfrom pyspark.sql.functions import regexp_replace\nfrom sklearn.metrics import calinski_harabasz_score, adjusted_rand_score"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df500c32-2488-41c4-b547-928c0f7d0d09"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def K_means(data,k,CT=0.0001,I=30):\n    \n    # scale the data\n    tovector = VectorAssembler(inputCols=data.columns[:-1], outputCol=\"features\").transform(data)     # convert each row to vector- create the function\n    pointsscale = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(tovector).transform(tovector).select(\"scaledFeatures\")      # create MinMaxScaler\n    array_points = pointsscale.withColumn(\"scaledFeatures\", vector_to_array(\"scaledFeatures\"))     # change back to list\n    points = array_points.rdd.map(lambda x: x[0]).map(lambda x: tuple(x))    # show only the vectors and convert to tuple\n       \n#   \n    def WCSS(point ,centroids):\n         # Saves the distance of the closest center and the center index\n        min_distance, index_center = float(\"inf\"),0\n        for i in range(len(centroids)):\n            lst = 0\n            for m in range(len(point)):\n                lst += (centroids[i][m] - point[m])**2\n            new_distance = lst**0.5   \n            if(new_distance < min_distance): # update min_distance to be the minimum distance from all the centers \n                min_distance, index_center = new_distance, i\n        return (index_center, point)\n    \n    \n    centroids = points.takeSample(False, k) # peak randomly the first samples \n    for i in range(I): \n        center_old = centroids\n        points_wcss = points.map(lambda x: WCSS(x,centroids))\n        lst_centers = points_wcss.map(lambda x: x[0])\n\n        # create new distance    \n        prapare_to_reduce = points_wcss.map(lambda x: (x[0], (x[1], 1)))   # Prapare to - reduce, the left value will be the key\n        avg_reduce = prapare_to_reduce.reduceByKey(lambda x,y: ([x1 + y1 for (x1, y1) in zip(x[0], y[0])], x[1] + y[1]))  # Count how much points in each center and sum together the rows\n        new_centers = avg_reduce.mapValues(lambda x: [(h / x[1]) for h in x[0]]).collect()  # Divide each value by the number of points to create the new center\n        \n        centroids = [i[1] for i in new_centers]\n        \n        # checks the distance between old to new center -less then CT\n        lst = 0\n        count =0\n        for i in range(len(center_old)):\n            count += 1\n            for m in range(len(center_old[0])):                \n                lst += (centroids[i][m] - center_old[i][m])**2\n                distance_oldnew = lst**0.5  # Distance between new to old\n            if distance_oldnew > CT: # if Distance bigger then CT continue\n                    break\n            if count == len(center_old):                             \n                return (new_centers, lst_centers.collect())  # stop and return\n            \n    return (new_centers, lst_centers.collect())\n       \n    \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccf72d47-79ab-476d-8a60-a4c90cc1a778"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def results(location, k, name, CT = 0.0001, I = 10, Exp = 10):\n    \n    data = location.toPandas()\n    true_clusterings = data['class'].tolist() # create list with the true centers\n    lst_ch = []\n    lst_ari = []\n    for i in range(Exp):  # foe each k iter 10 times\n\n        points = K_means(location, k, 0.0001,30)\n        pred_center = points[1]   # list of the new centers\n       \n        ari = adjusted_rand_score(true_clusterings, pred_center)# calcualte the ari measure\n        lst_ari.append(ari) #  append adjusted_rand_score to list\n        ch = calinski_harabasz_score(data, pred_center) # calcualte the ch measure\n        lst_ch.append(ch) # append calinski_harabasz_score to list\n       \n        \n    lst_ch = np.array(lst_ch)\n    mean_ch = np.mean(lst_ch)  # calculate the mean \n    std_ch = np.std(lst_ch)  # calculate the std\n    \n    lst_ari = np.array(lst_ari)\n    mean_ari = np.mean(lst_ari) # calculate the mean \n    std_ari = np.std(lst_ari)  # calculate the std \n    # print the results for each dataset\n    print(f'Dataset: {name} | K: {k} | CH: ({mean_ch:.2f}, {std_ch:.2f}) | ARI: ({mean_ari:.2f}, {std_ari:.2f}))')\n    \n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74ceedd4-de1e-47c2-a513-48b81240405d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfor k in range(2, 7):\n    results(iris, k, 'IRIS')\n\nfor k in range(2, 7):\n    results(glass, k, 'GLASS')\n    \nfor k in range(2, 7):\n    results(parkinsons, k, 'PARKINSON')\n  \n\n    \n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7f1f90a-d0b1-4379-8e56-178b3a74e98b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Dataset: IRIS | K: 2 | CH: (493.88, 0.00) | ARI: (0.57, 0.00))\nDataset: IRIS | K: 3 | CH: (496.73, 80.95) | ARI: (0.68, 0.08))\nDataset: IRIS | K: 4 | CH: (430.42, 51.65) | ARI: (0.60, 0.03))\nDataset: IRIS | K: 5 | CH: (391.20, 44.36) | ARI: (0.53, 0.05))\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Dataset: IRIS | K: 2 | CH: (493.88, 0.00) | ARI: (0.57, 0.00))\nDataset: IRIS | K: 3 | CH: (496.73, 80.95) | ARI: (0.68, 0.08))\nDataset: IRIS | K: 4 | CH: (430.42, 51.65) | ARI: (0.60, 0.03))\nDataset: IRIS | K: 5 | CH: (391.20, 44.36) | ARI: (0.53, 0.05))\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52a79782-5bd2-41c3-9ce4-afc25697ff96"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2022-06-12 - DBFS Example (2)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2863478849618872}},"nbformat":4,"nbformat_minor":0}
